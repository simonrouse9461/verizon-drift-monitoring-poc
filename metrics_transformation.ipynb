{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-03T23:50:25.111168Z",
     "start_time": "2023-06-03T23:50:25.064980Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Table, Column, MetaData, Integer, Computed, DateTime, Numeric, Float\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T23:50:25.658425Z",
     "start_time": "2023-06-03T23:50:25.466500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score, ROC, AUROC, PrecisionRecallCurve, AveragePrecision, ConfusionMatrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T23:50:29.252411Z",
     "start_time": "2023-06-03T23:50:25.862029Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from population_stability_index.psi import calculate_psi\n",
    "from lift_curve import lift_curve"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T23:50:29.329103Z",
     "start_time": "2023-06-03T23:50:29.253385Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, pandas_udf, monotonically_increasing_id, lit, array, PandasUDFType\n",
    "from pyspark.sql.types import IntegerType, TimestampType, FloatType, StructType, StructField, ArrayType, BooleanType"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T23:50:29.442711Z",
     "start_time": "2023-06-03T23:50:29.324320Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x19feb1e70>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://172.16.46.222:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.4.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[4]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master(\"local[4]\")\n",
    "         .config(key=\"spark.sql.caseSensitive\", value=True)\n",
    "         .config(key=\"spark.sql.execution.arrow.pyspark.fallback.enabled\", value=True)\n",
    "         .config(key=\"spark.sql.execution.arrow.pyspark.enabled\", value=True)\n",
    "         .config(key=\"spark.sql.execution.arrow.pyspark.datetime64.enabled\", value=True)\n",
    "         .config(key=\"spark.jars\", value=\"./jar/postgresql-42.6.0.jar\")\n",
    "         .getOrCreate())\n",
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T23:51:05.276568Z",
     "start_time": "2023-06-03T23:51:05.182420Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and Preprocess Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 16:36:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "23/06/02 16:36:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "23/06/02 16:36:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "23/06/02 16:36:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "23/06/02 16:36:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "23/06/02 16:36:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "23/06/02 16:36:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "23/06/02 16:36:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    }
   ],
   "source": [
    "baseline_data = spark.read.json(\"./datasets/baseline_data_predicted.jsonl\")\n",
    "realtime_data = spark.read.json(\"./datasets/realtime_data_predicted.jsonl\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:36:19.019073Z",
     "start_time": "2023-06-02T20:36:14.737687Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "num_shards = realtime_data.count() // 100\n",
    "timestamp_now = spark.sparkContext.broadcast(datetime.now())\n",
    "@udf(TimestampType())\n",
    "def generate_timestamp(shard_id):\n",
    "    return timestamp_now.value + timedelta(hours=shard_id - num_shards)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:36:19.479244Z",
     "start_time": "2023-06-02T20:36:18.999221Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "baseline_data = baseline_data.withColumn(\"timestamp\", lit(datetime.now()))\n",
    "realtime_data = realtime_data.withColumn(\"id\", monotonically_increasing_id())\n",
    "realtime_data = realtime_data.withColumn(\"shard_id\", realtime_data[\"id\"] % num_shards)\n",
    "realtime_data = realtime_data.withColumn(\"timestamp\", generate_timestamp(realtime_data[\"shard_id\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:36:19.662894Z",
     "start_time": "2023-06-02T20:36:19.461499Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set Up Database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "POSTGRES_PASSWORD = os.environ[\"POSTGRES_PASSWORD\"]\n",
    "POSTGRES_URL = f\"postgresql+psycopg://postgres:{POSTGRES_PASSWORD}@localhost/postgres\"\n",
    "engine = create_engine(POSTGRES_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "def get_db_options(table):\n",
    "    return dict(\n",
    "        url=f\"jdbc:postgresql://localhost:5432/postgres\",\n",
    "        dbtable=table,\n",
    "        user=\"postgres\",\n",
    "        password=POSTGRES_PASSWORD,\n",
    "        driver=\"org.postgresql.Driver\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:36:47.467354Z",
     "start_time": "2023-06-02T20:36:47.392968Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Common"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.10/site-packages/pyspark/sql/pandas/functions.py:399: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.10/site-packages/pyspark/sql/pandas/functions.py:399: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@udf(StructType([\n",
    "    StructField(f\"_{i}\", FloatType())\n",
    "    for i in range(100)\n",
    "]))\n",
    "def unpack_thresholds(thresholds: list, values: list) -> tuple:\n",
    "    result = [None] * 100\n",
    "    for t, v in zip(thresholds, values):\n",
    "        result[round(t * 100)] = v\n",
    "    return tuple(result)\n",
    "\n",
    "@udf(StructType([\n",
    "    StructField(f\"_{i}\", FloatType())\n",
    "    for i in range(100)\n",
    "]))\n",
    "def threshold_value_baseline_diff(baseline: list, *values: list) -> tuple:\n",
    "    return tuple(v - b for v, b in zip(values, baseline))\n",
    "\n",
    "@udf(StructType([\n",
    "    StructField(f\"decile_{i}\", FloatType())\n",
    "    for i in range(1, 11)\n",
    "]))\n",
    "def unpack_deciles(deciles: list, values: list) -> tuple:\n",
    "    result = [None] * 10\n",
    "    for d, v in zip(deciles, values):\n",
    "        result[round(d * 10) - 1] = v\n",
    "    return tuple(result)\n",
    "\n",
    "@udf(StructType([\n",
    "    StructField(f\"decile_{i}\", FloatType())\n",
    "    for i in range(1, 11)\n",
    "]))\n",
    "def decile_value_baseline_diff(baseline: list, *values: list) -> tuple:\n",
    "    return tuple(v - b for v, b in zip(values, baseline))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:36:26.590320Z",
     "start_time": "2023-06-02T20:36:25.586846Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AccuracyTable(Base := declarative_base()):\n",
    "    __tablename__ = 'acc_table'\n",
    "    __metricname__ = 'accuracy'\n",
    "\n",
    "    timestamp = Column(DateTime, primary_key=True)\n",
    "    baseline = Column(Float)\n",
    "    realtime = Column(Float)\n",
    "\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "@pandas_udf(FloatType(), PandasUDFType.GROUPED_AGG)\n",
    "def accuracy(score: pd.Series, label: pd.Series) -> float:\n",
    "    return Accuracy(task=\"binary\")(preds=torch.tensor(score), target=torch.tensor(label)).item()\n",
    "\n",
    "agg_expr = accuracy(\"score\", \"label\").alias(\"accuracy\")\n",
    "baseline_acc = baseline_data.agg(agg_expr).toPandas().iloc[0]\n",
    "realtime_acc = realtime_data.groupby(\"timestamp\").agg(agg_expr)\n",
    "\n",
    "(realtime_acc\n",
    " .select(\"timestamp\",\n",
    "         lit(baseline_acc[\"accuracy\"]).alias(\"baseline\"),\n",
    "         realtime_acc[\"accuracy\"].alias(\"realtime\"))\n",
    " .write.format('jdbc')\n",
    " .options(**get_db_options(\"acc_table\"))\n",
    " .mode(\"overwrite\")\n",
    " .save())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Precision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@pandas_udf(FloatType(), PandasUDFType.GROUPED_AGG)\n",
    "def precision(score: pd.Series, label: pd.Series) -> float:\n",
    "    return Precision(task=\"binary\")(preds=torch.tensor(score), target=torch.tensor(label)).item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recall"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@pandas_udf(FloatType(), PandasUDFType.GROUPED_AGG)\n",
    "def recall(score: pd.Series, label: pd.Series) -> float:\n",
    "    return Recall(task=\"binary\")(preds=torch.tensor(score), target=torch.tensor(label)).item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# F1 Score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@pandas_udf(FloatType(), PandasUDFType.GROUPED_AGG)\n",
    "def f1score(score: pd.Series, label: pd.Series) -> float:\n",
    "    return F1Score(task=\"binary\")(preds=torch.tensor(score), target=torch.tensor(label)).item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AUROC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@pandas_udf(FloatType(), PandasUDFType.GROUPED_AGG)\n",
    "def auroc(score: pd.Series, label: pd.Series) -> float:\n",
    "    return AUROC(task=\"binary\")(preds=torch.tensor(score), target=torch.tensor(label)).item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AUPRC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@pandas_udf(FloatType(), PandasUDFType.GROUPED_AGG)\n",
    "def auprc(score: pd.Series, label: pd.Series) -> float:\n",
    "    return AveragePrecision(task=\"binary\")(preds=torch.tensor(score), target=torch.tensor(label)).item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Confusion Matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@pandas_udf(ArrayType(FloatType()), PandasUDFType.GROUPED_AGG)\n",
    "def confusion(score: pd.Series, label: pd.Series) -> list[float]:\n",
    "    confmat = ConfusionMatrix(task=\"binary\")(preds=torch.tensor(score), target=torch.tensor(label))\n",
    "    return confmat.flatten().tolist()\n",
    "\n",
    "@udf(StructType([\n",
    "    StructField(\"tn\", FloatType()),\n",
    "    StructField(\"fn\", FloatType()),\n",
    "    StructField(\"fp\", FloatType()),\n",
    "    StructField(\"tp\", FloatType()),\n",
    "]))\n",
    "def confusion_struct(confmat: list) -> tuple:\n",
    "    return tuple(confmat)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ROC Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@pandas_udf(ArrayType(ArrayType(FloatType())), PandasUDFType.GROUPED_AGG)\n",
    "def roccurve(score: pd.Series, label: pd.Series) -> list[float]:\n",
    "    roc = ROC(task=\"binary\", thresholds=torch.arange(0, 1, 0.01))(preds=torch.tensor(score), target=torch.tensor(label))\n",
    "    return [c.tolist() for c in roc]\n",
    "\n",
    "@udf(StructType([\n",
    "    StructField(\"rocfpr\", ArrayType(FloatType())),\n",
    "    StructField(\"roctpr\", ArrayType(FloatType())),\n",
    "    StructField(\"rocthresh\", ArrayType(FloatType())),\n",
    "]))\n",
    "def roccurve_struct(roc: list) -> tuple:\n",
    "    return tuple(roc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PR Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@pandas_udf(ArrayType(ArrayType(FloatType())), PandasUDFType.GROUPED_AGG)\n",
    "def prccurve(score: pd.Series, label: pd.Series) -> list[float]:\n",
    "    prc = PrecisionRecallCurve(task=\"binary\", thresholds=torch.arange(0, 1, 0.01))(preds=torch.tensor(score), target=torch.tensor(label))\n",
    "    return [c.tolist() for c in prc]\n",
    "\n",
    "@udf(StructType([\n",
    "    StructField(\"prcprec\", ArrayType(FloatType())),\n",
    "    StructField(\"prcrec\", ArrayType(FloatType())),\n",
    "    StructField(\"prcthresh\", ArrayType(FloatType())),\n",
    "]))\n",
    "def prccurve_struct(roc: list) -> tuple:\n",
    "    return tuple(roc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lift Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@pandas_udf(ArrayType(ArrayType(FloatType())), PandasUDFType.GROUPED_AGG)\n",
    "def liftcurve(score: pd.Series, label: pd.Series) -> list[float]:\n",
    "    lift = lift_curve(y_val=torch.tensor(label), y_pred=torch.tensor(score), step=0.1)\n",
    "    return [c.tolist() for c in lift]\n",
    "\n",
    "@udf(StructType([\n",
    "    StructField(\"decile\", ArrayType(FloatType())),\n",
    "    StructField(\"lift\", ArrayType(FloatType())),\n",
    "]))\n",
    "def liftcurve_struct(roc: list) -> tuple:\n",
    "    return tuple(roc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class LiftTable(Base):\n",
    "    __tablename__ = 'lift_table'\n",
    "\n",
    "    timestamp = Column(DateTime, primary_key=True)\n",
    "\n",
    "for decile in range(1, 11):\n",
    "    setattr(LiftTable, f\"decile_{decile}\", Column(Float))\n",
    "\n",
    "class LiftDiffTable(Base):\n",
    "    __tablename__ = 'lift_diff_table'\n",
    "\n",
    "    timestamp = Column(DateTime, primary_key=True)\n",
    "\n",
    "for decile in range(1, 11):\n",
    "    setattr(LiftDiffTable, f\"decile_{decile}\", Column(Float))\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_values = list(unpack_deciles.func(baseline_metrics[\"decile\"], baseline_metrics[\"lift\"]))\n",
    "value = (realtime_metrics\n",
    "         .select(\"timestamp\", unpack_deciles(\"decile\", \"lift\").alias(\"struct\"))\n",
    "         .select(\"timestamp\", \"struct.*\")).cache()\n",
    "diff = (value\n",
    "        .select(\"timestamp\", decile_value_baseline_diff(lit(baseline_values), *[f\"decile_{i}\" for i in range(1, 11)]).alias(\"struct\"))\n",
    "        .select(\"timestamp\", \"struct.*\"))\n",
    "(value\n",
    " .write.format('jdbc')\n",
    " .options(**get_db_options(LiftTable.__tablename__))\n",
    " .mode(\"overwrite\")\n",
    " .save())\n",
    "(diff\n",
    " .write.format('jdbc')\n",
    " .options(**get_db_options(LiftDiffTable.__tablename__))\n",
    " .mode(\"overwrite\")\n",
    " .save())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PSI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class PSITable(Base):\n",
    "    __tablename__ = \"psi_table\"\n",
    "    __metricname__ = \"psi\"\n",
    "\n",
    "    timestamp = Column(DateTime, primary_key=True)\n",
    "    value = Column(Float)\n",
    "\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "baseline_score = spark.sparkContext.broadcast(baseline_data.select(\"score\").toPandas().to_numpy().flatten())\n",
    "@pandas_udf(FloatType(), PandasUDFType.GROUPED_AGG)\n",
    "def psi(score: pd.Series) -> float:\n",
    "    return calculate_psi(baseline_score.value, np.array(score))\n",
    "\n",
    "\n",
    "(realtime_metrics\n",
    " .select(\"timestamp\", realtime_metrics[PSITable.__metricname__].alias(\"value\"))\n",
    " .write.format('jdbc')\n",
    " .options(**get_db_options(PSITable.__tablename__))\n",
    " .mode(\"overwrite\")\n",
    " .save())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transform Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def eval_performance(dataset, compute_psi=False):\n",
    "    args = dataset[\"score\"], dataset[\"label\"]\n",
    "    agg_fns = [\n",
    "        accuracy(*args).alias(\"accuracy\"),\n",
    "        precision(*args).alias(\"precision\"),\n",
    "        recall(*args).alias(\"recall\"),\n",
    "        f1score(*args).alias(\"f1score\"),\n",
    "        confusion(*args).alias(\"confusion\"),\n",
    "        roccurve(*args).alias(\"roccurve\"),\n",
    "        auroc(*args).alias(\"auroc\"),\n",
    "        prccurve(*args).alias(\"prccurve\"),\n",
    "        auprc(*args).alias(\"auprc\"),\n",
    "        liftcurve(*args).alias(\"liftcurve\"),\n",
    "    ]\n",
    "    if compute_psi:\n",
    "        agg_fns.append(psi(args[0]).alias(\"psi\"))\n",
    "    metrics = (dataset\n",
    "               .groupby(\"timestamp\")\n",
    "               .agg(*agg_fns)\n",
    "               .withColumn(\"confusion_struct\", confusion_struct(\"confusion\"))\n",
    "               .withColumn(\"roccurve_struct\", roccurve_struct(\"roccurve\"))\n",
    "               .withColumn(\"prccurve_struct\", prccurve_struct(\"prccurve\"))\n",
    "               .withColumn(\"liftcurve_struct\", liftcurve_struct(\"liftcurve\"))\n",
    "               .select(\"*\", \"confusion_struct.*\", \"roccurve_struct.*\", \"prccurve_struct.*\", \"liftcurve_struct.*\")\n",
    "               .drop(\"confusion\", \"roccurve\", \"prccurve\", \"liftcurve\")\n",
    "               .drop(\"confusion_struct\", \"roccurve_struct\", \"prccurve_struct\", \"liftcurve_struct\"))\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:36:29.673683Z",
     "start_time": "2023-06-02T20:36:29.605715Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "baseline_metrics = eval_performance(baseline_data).toPandas().iloc[0]\n",
    "realtime_metrics = eval_performance(realtime_data, compute_psi=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:36:40.627987Z",
     "start_time": "2023-06-02T20:36:30.993625Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Push Basic Metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PrecisionTable(Base):\n",
    "    __tablename__ = 'prec_table'\n",
    "    __metricname__ = 'precision'\n",
    "\n",
    "    timestamp = Column(DateTime, primary_key=True)\n",
    "    baseline = Column(Float)\n",
    "    realtime = Column(Float)\n",
    "\n",
    "class RecallTable(Base):\n",
    "    __tablename__ = 'rec_table'\n",
    "    __metricname__ = 'recall'\n",
    "\n",
    "    timestamp = Column(DateTime, primary_key=True)\n",
    "    baseline = Column(Float)\n",
    "    realtime = Column(Float)\n",
    "\n",
    "class F1Table(Base):\n",
    "    __tablename__ = 'f1_table'\n",
    "    __metricname__ = 'f1score'\n",
    "\n",
    "    timestamp = Column(DateTime, primary_key=True)\n",
    "    baseline = Column(Float)\n",
    "    realtime = Column(Float)\n",
    "\n",
    "class AUROCTable(Base):\n",
    "    __tablename__ = 'auroc_table'\n",
    "    __metricname__ = 'auroc'\n",
    "\n",
    "    timestamp = Column(DateTime, primary_key=True)\n",
    "    baseline = Column(Float)\n",
    "    realtime = Column(Float)\n",
    "\n",
    "class AUPRCTable(Base):\n",
    "    __tablename__ = 'auprc_table'\n",
    "    __metricname__ = 'auprc'\n",
    "\n",
    "    timestamp = Column(DateTime, primary_key=True)\n",
    "    baseline = Column(Float)\n",
    "    realtime = Column(Float)\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:37:15.487599Z",
     "start_time": "2023-06-02T20:37:15.246483Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 16:37:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for table in AccuracyTable, PrecisionTable, RecallTable, F1Table, AUROCTable, AUPRCTable:\n",
    "    (realtime_metrics\n",
    "     .select(\"timestamp\",\n",
    "             lit(baseline_metrics[table.__metricname__]).alias(\"baseline\"),\n",
    "             realtime_metrics[table.__metricname__].alias(\"realtime\"))\n",
    "     .write.format('jdbc')\n",
    "     .options(**get_db_options(table.__tablename__))\n",
    "     .mode(\"overwrite\")\n",
    "     .save())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:37:40.200191Z",
     "start_time": "2023-06-02T20:37:17.495551Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Push ROC and PRC Curves"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "def make_curve_table_class(type_name, table_name, metric_name, thresh_name, thresh_interval=1):\n",
    "    return type(type_name, (Base,), dict(\n",
    "        __tablename__=table_name,\n",
    "        __metricname__=metric_name,\n",
    "        __threshname__=thresh_name,\n",
    "        timestamp=Column(DateTime, primary_key=True),\n",
    "    ) | {f\"_{i}\": Column(Float) for i in range(0, 100, thresh_interval)})\n",
    "\n",
    "thresh_interval = 1\n",
    "\n",
    "table_metadata = {\n",
    "    \"ROCFPR\": {\n",
    "        \"table_prefix\": \"roc_fp\",\n",
    "        \"metric_name\": \"rocfpr\",\n",
    "        \"thresh_name\": \"rocthresh\"\n",
    "    },\n",
    "    \"ROCTPR\": {\n",
    "        \"table_prefix\": \"roc_tp\",\n",
    "        \"metric_name\": \"roctpr\",\n",
    "        \"thresh_name\": \"rocthresh\"\n",
    "    },\n",
    "    \"PRCPrecision\": {\n",
    "        \"table_prefix\": \"prc_prec\",\n",
    "        \"metric_name\": \"prcprec\",\n",
    "        \"thresh_name\": \"prcthresh\"\n",
    "    },\n",
    "    \"PRCRecall\": {\n",
    "        \"table_prefix\": \"prc_rec\",\n",
    "        \"metric_name\": \"prcrec\",\n",
    "        \"thresh_name\": \"prcthresh\"\n",
    "    }\n",
    "}\n",
    "\n",
    "tables = {\n",
    "    prefix: (\n",
    "        make_curve_table_class(\n",
    "            type_name=prefix + \"Table\",\n",
    "            table_name=meta[\"table_prefix\"] + \"_table\",\n",
    "            metric_name=meta[\"metric_name\"],\n",
    "            thresh_name=meta[\"thresh_name\"],\n",
    "            thresh_interval=thresh_interval\n",
    "        ),\n",
    "        make_curve_table_class(\n",
    "            type_name=prefix + \"DiffTable\",\n",
    "            table_name=meta[\"table_prefix\"] + \"_diff_table\",\n",
    "            metric_name=meta[\"metric_name\"],\n",
    "            thresh_name=meta[\"thresh_name\"],\n",
    "            thresh_interval=thresh_interval\n",
    "        )\n",
    "    ) for prefix, meta in table_metadata.items()\n",
    "}\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for value_table, diff_table in tables.values():\n",
    "    threshname = value_table.__threshname__\n",
    "    metricname = value_table.__metricname__\n",
    "    baseline_values = list(unpack_thresholds.func(baseline_metrics[threshname], baseline_metrics[metricname]))\n",
    "    value = (realtime_metrics\n",
    "             .select(\"timestamp\", unpack_thresholds(threshname, metricname).alias(\"struct\"))\n",
    "             .select(\"timestamp\", \"struct.*\")).cache()\n",
    "    diff = (value\n",
    "            .select(\"timestamp\", threshold_value_baseline_diff(lit(baseline_values), *[f\"_{i}\" for i in range(100)]).alias(\"struct\"))\n",
    "            .select(\"timestamp\", \"struct.*\"))\n",
    "    (value\n",
    "     .write.format('jdbc')\n",
    "     .options(**get_db_options(value_table.__tablename__))\n",
    "     .mode(\"overwrite\")\n",
    "     .save())\n",
    "    (diff\n",
    "     .write.format('jdbc')\n",
    "     .options(**get_db_options(diff_table.__tablename__))\n",
    "     .mode(\"overwrite\")\n",
    "     .save())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Push Model Prediction Scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class PositiveScoreTable(Base):\n",
    "    __tablename__ = 'pos_score_table'\n",
    "\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    timestamp = Column(DateTime)\n",
    "    score = Column(Float)\n",
    "\n",
    "class NegativeScoreTable(Base):\n",
    "    __tablename__ = 'neg_score_table'\n",
    "\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    timestamp = Column(DateTime)\n",
    "    score = Column(Float)\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:37:41.371087Z",
     "start_time": "2023-06-02T20:37:41.286322Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "@udf(BooleanType())\n",
    "def filter_positive(label):\n",
    "    return label == 1\n",
    "\n",
    "@udf(BooleanType())\n",
    "def filter_negative(label):\n",
    "    return label == 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:37:41.439634Z",
     "start_time": "2023-06-02T20:37:41.372104Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(realtime_data\n",
    " .filter(filter_positive(\"label\"))\n",
    " .select(\"timestamp\", \"score\")\n",
    " .write.format('jdbc')\n",
    " .options(**get_db_options(PositiveScoreTable.__tablename__))\n",
    " .mode(\"overwrite\")\n",
    " .save())\n",
    "(realtime_data\n",
    " .filter(filter_negative(\"label\"))\n",
    " .select(\"timestamp\", \"score\")\n",
    " .write.format('jdbc')\n",
    " .options(**get_db_options(NegativeScoreTable.__tablename__))\n",
    " .mode(\"overwrite\")\n",
    " .save())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:37:43.291456Z",
     "start_time": "2023-06-02T20:37:41.441316Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Push Confusion Matrices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class ConfusionTable(Base):\n",
    "    __tablename__ = 'confusion_table'\n",
    "\n",
    "    timestamp = Column(DateTime, primary_key=True)\n",
    "    tp = Column(Float)\n",
    "    fp = Column(Float)\n",
    "    tn = Column(Float)\n",
    "    fn = Column(Float)\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:37:44.366729Z",
     "start_time": "2023-06-02T20:37:44.267322Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(realtime_metrics\n",
    " .select(\"timestamp\", \"tp\", \"fp\", \"tn\", \"fn\")\n",
    " .write.format('jdbc')\n",
    " .options(**get_db_options(ConfusionTable.__tablename__))\n",
    " .mode(\"overwrite\")\n",
    " .save())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:37:51.951278Z",
     "start_time": "2023-06-02T20:37:45.302643Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
