version: '3'

vars:
  GCP_PROJECT: 'healthy-earth-389717'
  GCP_REGION: 'us-central1'
  GCP_DATAPROC_CLUSTER: 'verizon-drift-monitoring'
  SPARK_JOB_DOCKER_IMAGE: 'us-central1-docker.pkg.dev/healthy-earth-389717/verizon-drift-monitoring/spark-job-env:latest'
  SPARK_JOB_PROPERTIES: 'spark-job.properties'
  SPARK_JOB_DOTENV_FILE: '.env.dataproc'

tasks:
  build:
    cmds:
      - docker build --platform linux/amd64 -t {{.SPARK_JOB_DOCKER_IMAGE}} .

  push:
    cmds:
      - docker push {{.SPARK_JOB_DOCKER_IMAGE}}

  deploy:
    cmds:
      - python create_database_tables.py {{.CLI_ARGS}}
      - python create_dashboard_json.py {{.CLI_ARGS}}

  zip:
    cmds:
      - zip -r src.zip src/

  generate-properties:
    cmds:
      - jinja2 --format=yml {{.SPARK_JOB_PROPERTIES}}.j2 Taskfile.yml > {{.SPARK_JOB_PROPERTIES}}

  transform:
    cmds:
      - python spark_transform.py {{.CLI_ARGS}}

  submit-transform:
    cmds:
      - task: zip
      - task: generate-properties
      - gcloud dataproc --project={{.GCP_PROJECT}}
        jobs submit pyspark spark_transform.py
        --cluster={{.GCP_DATAPROC_CLUSTER}}
        --region={{.GCP_REGION}}
        --properties-file={{.SPARK_JOB_PROPERTIES}}
        --files={{.SPARK_JOB_DOTENV_FILE}}
        --py-files=src.zip
        -- {{.CLI_ARGS}}

  submit-test:
    cmds:
      - task: zip
      - gcloud dataproc --project={{.GCP_PROJECT}}
        jobs submit pyspark spark_test.py
        --cluster={{.GCP_DATAPROC_CLUSTER}}
        --region={{.GCP_REGION}}
        --properties-file={{.SPARK_JOB_PROPERTIES}}
        --files={{.SPARK_JOB_DOTENV_FILE}}
        --py-files=src.zip

  test:
    cmds:
      - echo {{.CLI_ARGS}}
      - echo {{.SHELL}}